{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5f777a7-0adf-40f3-9b94-a9300335085a",
   "metadata": {},
   "source": [
    "# **The Many Faces of Hurricanes: The Influence of AMO, ENSO, and Higher-Order Moment Analysis**\n",
    "\n",
    "## **Introduction**\n",
    "> *\"If you know where the wind comes from, you can predict where it will take you.\"*  \n",
    "> — A Meteorologist  \n",
    "\n",
    "Hurricanes, one of the most destructive natural phenomena on Earth 🌍, have shown changes in frequency, intensity, and trajectory under global warming. Scientists have long been investigating these variations, seeking to understand the key climate drivers behind them.  \n",
    "\n",
    "This study focuses on **the Atlantic Multidecadal Oscillation (AMO) and the El Niño-Southern Oscillation (ENSO)**, combining **hurricane higher-order moment (mass moments) analysis with K-Means clustering** to uncover patterns in hurricane behavior and anticipate future trends.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b77b0ed-b077-481c-a708-fdfdcedbf0aa",
   "metadata": {},
   "source": [
    "## **1. The Climate Signals Behind Hurricanes**\n",
    "The formation and development of hurricanes are influenced by multiple large-scale climate factors, with **AMO and ENSO being two of the most critical**.\n",
    "\n",
    "### **1.1 📖 What is AMO (Atlantic Multidecadal Oscillation)?**\n",
    "AMO refers to **long-term oscillations in North Atlantic sea surface temperature (SST)**, typically lasting **20-40 years**. It has two phases:\n",
    "- **Positive Phase (Warm Cycle)**: Warmer-than-average SST in the North Atlantic.\n",
    "  - Increases hurricane activity by providing more heat and moisture.\n",
    "  - Leads to stronger and more frequent hurricanes.\n",
    "  - Causes hurricanes to follow a more **westward** trajectory, making landfall in North America more likely.\n",
    "\n",
    "- **Negative Phase (Cold Cycle)**: Cooler-than-average SST in the North Atlantic.\n",
    "  - Suppresses hurricane development.\n",
    "  - Results in fewer hurricanes, generally weaker in intensity.\n",
    "  - Causes hurricanes to stay farther east over the ocean, reducing landfall probability.\n",
    "\n",
    "### **1.2 📖 What is ENSO (El Niño-Southern Oscillation)?**\n",
    "ENSO is a **climate phenomenon in the tropical Pacific** that influences global atmospheric circulation. It has two major phases:\n",
    "- **El Niño (Warm Phase)**:\n",
    "  - Warmer SST in the central and eastern Pacific.\n",
    "  - Increases wind shear over the Atlantic, **suppressing hurricane activity**.\n",
    "  - Leads to **fewer hurricanes**, weaker storms, and eastward-shifted hurricane tracks.\n",
    "\n",
    "- **La Niña (Cold Phase)**:\n",
    "  - Cooler SST in the central and eastern Pacific.\n",
    "  - Reduces wind shear, **enhancing hurricane development**.\n",
    "  - Leads to **more hurricanes**, stronger storms, and westward-shifted hurricane tracks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74af891-5208-421b-8360-d227282bbd70",
   "metadata": {},
   "source": [
    "### Next, let's figure out whether AMO or ENSO has a greater impact on hurricanes. \n",
    "### Next Station ➡️ Part 2 ！"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389beb57-db44-479b-b375-e88ccc718a26",
   "metadata": {},
   "source": [
    "## **2.AMO Data and Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fce335b-745b-46ed-b3e5-2a1c7c5eb189",
   "metadata": {},
   "source": [
    "Before we start to the AMO code, let's first guess what effect the AMO will have on hurricanes. 🌬️ 🌊\n",
    "- Will the warm phasenuan 🔥 increase or decrease the number of hurricanes?\n",
    "- What is the hurricane grade?\n",
    "- What about the cold phase 🧊?\n",
    "\n",
    "Have you guessed them? 🤔 Now, let's find them out!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17338182-bd9c-4706-a833-df9a582bdaf3",
   "metadata": {},
   "source": [
    "#### **2.1 AMO Research Code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "81195884-2aa9-4280-92cc-55d360371497",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "cwd=os.getcwd()\n",
    "\n",
    "cwd_data=cwd+'/data'\n",
    "\n",
    "if not os.path.exists(cwd_data):\n",
    "    os.mkdir(cwd_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "51118951-6e87-4237-bb04-5f0d8f8e61a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# These uninstallation must be done to ensure that no version conflicts would happen.\n",
    "!pip uninstall --yes cartopy\n",
    "!pip uninstall --yes shapely\n",
    "# Shapely and cartopy are used for graphing maps and tracks.\n",
    "!pip install shapely cartopy\n",
    "!pip install gender_guesser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1ee5ff1a-3301-47aa-8ad1-7ae59dbd5e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cartopy.crs as ccrs # used for map projection\n",
    "import matplotlib.pyplot as plt # matplotlib\n",
    "import cartopy.feature as cfeature # used for map projection\n",
    "import xarray as xr # x-array\n",
    "import numpy as np # numpy\n",
    "import urllib.request # download request\n",
    "import warnings # to suppress warnings\n",
    "import gender_guesser.detector as gender # for analyzing the names of hurricanes\n",
    "from numpy import linalg as LA # to plot the moments (by calculating the eigenvalues)\n",
    "from sklearn.cluster import k_means # to perform k-means\n",
    "from collections import Counter # set operations\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "88178676-9e70-4713-9fc2-291cc928869f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File downloaded successfully.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# IBTrACS.NA.v04r00.nc presents data from 1842-10-25 through 2023-06-07 \n",
    "url = 'https://www.ncei.noaa.gov/data/international-best-track-archive-for-climate-stewardship-ibtracs/v04r00/access/netcdf/IBTrACS.NA.v04r00.nc'\n",
    "output_file = 'data/NA_data.nc'\n",
    "\n",
    "try:\n",
    "    response = requests.get(url, timeout=60)\n",
    "    response.raise_for_status()  # Raise an error for HTTP codes >= 400\n",
    "    with open(output_file, 'wb') as f:\n",
    "        f.write(response.content)\n",
    "    print(\"File downloaded successfully.\")\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"Request failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1ffcd349-718e-4f05-80fb-8db044270aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the needed track file\n",
    "filedata = urllib.request.urlopen('https://www.ncei.noaa.gov/data/international-best-track-archive-for-climate-stewardship-ibtracs/v04r00/access/netcdf/IBTrACS.NA.v04r00.nc')\n",
    "\n",
    "datatowrite = filedata.read()\n",
    "\n",
    "with open('data/NA_data.nc', 'wb') as f:   \n",
    "    f.write(datatowrite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2258e741-9c95-4be0-bb97-55461842352a",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'ersst.v5.amo.dat.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 77\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m# Use functions\u001b[39;00m\n\u001b[1;32m     76\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mersst.v5.amo.dat.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Replace with your file path\u001b[39;00m\n\u001b[0;32m---> 77\u001b[0m AMO_time \u001b[38;5;241m=\u001b[39m \u001b[43mload_and_process_amo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m fig \u001b[38;5;241m=\u001b[39m plot_amo_index(AMO_time)\n\u001b[1;32m     79\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "Cell \u001b[0;32mIn[19], line 9\u001b[0m, in \u001b[0;36mload_and_process_amo\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_and_process_amo\u001b[39m(file_path):\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# Read AMO data\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m     AMO_time \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43ms+\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskiprows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mYear\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMonth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSSTA\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# Convert data types\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     AMO_time \u001b[38;5;241m=\u001b[39m AMO_time\u001b[38;5;241m.\u001b[39mastype({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYear\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mint\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMonth\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mint\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSSTA\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mfloat\u001b[39m})\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.12/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'ersst.v5.amo.dat.txt'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.dates import YearLocator, DateFormatter\n",
    "\n",
    "# Load and calculate 10-year moving average\n",
    "def load_and_process_amo(file_path):\n",
    "    # Read AMO data\n",
    "    AMO_time = pd.read_csv(file_path, sep=r'\\s+', skiprows=2, names=[\"Year\", \"Month\", \"SSTA\"])\n",
    "    \n",
    "    # Convert data types\n",
    "    AMO_time = AMO_time.astype({\"Year\": int, \"Month\": int, \"SSTA\": float})\n",
    "    \n",
    "    # Create date column\n",
    "    AMO_time[\"Date\"] = pd.to_datetime(AMO_time[\"Year\"].astype(str) + \"-\" + \n",
    "                                    AMO_time[\"Month\"].astype(str).str.zfill(2))\n",
    "    \n",
    "    # Calculate 10-year moving average (121 months)\n",
    "    AMO_time[\"AMO_Smoothed\"] = AMO_time[\"SSTA\"].rolling(window=121, center=True).mean()\n",
    "    \n",
    "    # Remove missing values\n",
    "    AMO_time = AMO_time.dropna(subset=[\"AMO_Smoothed\"])\n",
    "    \n",
    "    return AMO_time\n",
    "\n",
    "def plot_amo_index(AMO_time):\n",
    "    # Find zero crossings\n",
    "    zero_crossings = np.where(np.diff(np.sign(AMO_time['AMO_Smoothed'])))[0]\n",
    "    zero_dates = AMO_time.iloc[zero_crossings]['Date']\n",
    "    \n",
    "    # Create figure\n",
    "    fig, ax = plt.subplots(figsize=(15, 5))\n",
    "    \n",
    "    # Plot filled areas\n",
    "    ax.fill_between(AMO_time['Date'], AMO_time['AMO_Smoothed'], 0,\n",
    "                    where=(AMO_time['AMO_Smoothed'] > 0),\n",
    "                    color='red', alpha=0.7, label='Warm Phase')\n",
    "    ax.fill_between(AMO_time['Date'], AMO_time['AMO_Smoothed'], 0,\n",
    "                    where=(AMO_time['AMO_Smoothed'] <= 0),\n",
    "                    color='blue', alpha=0.7, label='Cool Phase')\n",
    "    \n",
    "    # Plot black line\n",
    "    ax.plot(AMO_time['Date'], AMO_time['AMO_Smoothed'], color='black', linewidth=1)\n",
    "    \n",
    "    # Add vertical lines and labels at zero crossings\n",
    "    for date in zero_dates:\n",
    "        ax.axvline(date, color='gray', linestyle='dashed', alpha=0.6)\n",
    "        # Format date label as \"YYYY-MM\"\n",
    "        date_label = date.strftime('%Y-%m')\n",
    "        ax.text(date, 0.05, date_label, rotation=90, \n",
    "                verticalalignment='bottom', fontsize=10)\n",
    "    \n",
    "    # Set title and style\n",
    "    ax.set_title(\"Atlantic Multi-Decadal Oscillation (AMO) Index\\n\" + \n",
    "                 \"Smoothed Trend (10-year Rolling Mean)\", \n",
    "                 fontsize=16, fontweight=\"bold\", pad=20)\n",
    "    \n",
    "    # Set x-axis\n",
    "    ax.xaxis.set_major_locator(YearLocator(20))  # One tick every 20 years\n",
    "    ax.xaxis.set_major_formatter(DateFormatter(\"%Y\"))\n",
    "    \n",
    "    # Set figure limits\n",
    "    ax.set_xlim(AMO_time['Date'].min(), AMO_time['Date'].max())\n",
    "    ax.set_ylim(-1.5, 1.5)\n",
    "    \n",
    "    # Add grid and legend\n",
    "    ax.grid(True, alpha=0.3, linestyle=\"--\")\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "    ax.legend(loc=\"upper left\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# Use functions\n",
    "file_path = \"ersst.v5.amo.dat.txt\"  # Replace with your file path\n",
    "AMO_time = load_and_process_amo(file_path)\n",
    "fig = plot_amo_index(AMO_time)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e938ff-b97a-487c-b135-5fee1691db83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def categorize_hurricane(max_wind_speed):\n",
    "    \"\"\"Classify hurricanes based on maximum wind speed\"\"\"\n",
    "    if 34 <= max_wind_speed < 64:\n",
    "        return \"Tropical Storm\"\n",
    "    elif max_wind_speed < 83:\n",
    "        return \"Category 1\"\n",
    "    elif max_wind_speed < 96:\n",
    "        return \"Category 2\"\n",
    "    elif max_wind_speed < 113:\n",
    "        return \"Category 3\"\n",
    "    elif max_wind_speed < 137:\n",
    "        return \"Category 4\"\n",
    "    else:\n",
    "        return \"Category 5\"\n",
    "\n",
    "def plot_phase_comparison(df, save_path=None):\n",
    "    \"\"\"\n",
    "    Plot phase comparison between cold and warm phases\n",
    "    \"\"\"\n",
    "    # Create a 6x2 grid of subplots\n",
    "    fig = plt.figure(figsize=(20, 30))\n",
    "    \n",
    "    # Define categories and colors\n",
    "    category_colors = {\n",
    "        \"Tropical Storm\": (144/255, 238/255, 144/255),    # Light green\n",
    "        \"Category 1\": (238/255, 232/255, 170/255),        # Light yellow\n",
    "        \"Category 2\": (244/255, 164/255, 96/255),         # Light orange\n",
    "        \"Category 3\": (233/255, 150/255, 122/255),        # Light red\n",
    "        \"Category 4\": (221/255, 160/255, 221/255),        # Light purple\n",
    "        \"Category 5\": (147/255, 112/255, 219/255)         # Dark purple\n",
    "    }\n",
    "    \n",
    "    categories = [\"Category 5\", \"Category 4\", \"Category 3\", \n",
    "                  \"Category 2\", \"Category 1\", \"Tropical Storm\"]\n",
    "    phases = [\"Cold phase\", \"Warm phase\"]\n",
    "    \n",
    "    for cat_idx, category in enumerate(categories):\n",
    "        for phase_idx, phase in enumerate(phases):\n",
    "            ax = plt.subplot(6, 2, cat_idx*2 + phase_idx + 1, projection=ccrs.PlateCarree())\n",
    "            \n",
    "            # Set map extent\n",
    "            ax.set_extent([-100, 0, 0, 60], crs=ccrs.PlateCarree())\n",
    "            \n",
    "            # Add natural geographic features\n",
    "            ax.add_feature(cfeature.NaturalEarthFeature('physical', 'ocean', '50m', \n",
    "                                                         edgecolor='face', \n",
    "                                                         facecolor=cfeature.COLORS['water']))\n",
    "            ax.add_feature(cfeature.NaturalEarthFeature('physical', 'land', '50m', \n",
    "                                                         edgecolor='face',\n",
    "                                                         facecolor=cfeature.COLORS['land']))\n",
    "            ax.add_feature(cfeature.BORDERS, linestyle=':')\n",
    "            ax.add_feature(cfeature.COASTLINE)\n",
    "            \n",
    "            # Add gridlines\n",
    "            gl = ax.gridlines(draw_labels=True, linewidth=0.5, color='gray', \n",
    "                              alpha=0.5, linestyle='--')\n",
    "            gl.top_labels = False\n",
    "            gl.right_labels = False if phase_idx == 0 else False\n",
    "            gl.left_labels = True if phase_idx == 0 else False\n",
    "            \n",
    "            # Filter data\n",
    "            storm_ids = df[df['Category'] == category]['Storm_ID'].unique()\n",
    "            storm_data = df[df['Storm_ID'].isin(storm_ids) & (df['AMO_Phase'] == phase)]\n",
    "            \n",
    "            if len(storm_data) > 0:\n",
    "                plotted_storms = set()\n",
    "                for storm_id in storm_data['Storm_ID'].unique():\n",
    "                    if storm_id in plotted_storms:\n",
    "                        continue\n",
    "                        \n",
    "                    track = df[df['Storm_ID'] == storm_id].sort_values('ISO_Time')\n",
    "                    track = track.iloc[::16, :]  # Reduce point density\n",
    "                    \n",
    "                    # Plot track line\n",
    "                    ax.plot(track['Longitude'], track['Latitude'],\n",
    "                           color='black',\n",
    "                           linewidth=0.8,\n",
    "                           alpha=0.4,\n",
    "                           zorder=2)\n",
    "                    \n",
    "                    # Add points\n",
    "                    ax.scatter(track['Longitude'], track['Latitude'],\n",
    "                               color=category_colors[category],\n",
    "                               s=15,\n",
    "                               alpha=0.6,\n",
    "                               edgecolor='black',\n",
    "                               linewidth=0.5,\n",
    "                               zorder=3)\n",
    "                    \n",
    "                    plotted_storms.add(storm_id)\n",
    "            \n",
    "            # Add subplot title\n",
    "            storm_count = len(set(storm_data['Storm_ID']))\n",
    "            ax.set_title(f\"{category} - {phase}\\n({storm_count} storms)\", \n",
    "                         fontsize=12, pad=10)\n",
    "    \n",
    "    # Add overall title\n",
    "    plt.suptitle(\"1950-2020 North Atlantic Hurricanes\\nPhase Comparison\", \n",
    "                 fontsize=16, y=0.92)\n",
    "    \n",
    "    # Adjust subplot spacing\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "def main():\n",
    "    # Read data\n",
    "    df = pd.read_csv('updated_storm_data.csv')\n",
    "    \n",
    "    # Handle date format more flexibly\n",
    "    try:\n",
    "        df['Year'] = pd.to_datetime(df['ISO_Time'], format='mixed').dt.year\n",
    "    except:\n",
    "        # If the above method fails, try extracting the year directly from Storm_ID\n",
    "        df['Year'] = df['Storm_ID'].str[:16].astype(int)\n",
    "    \n",
    "    # Restrict time range to 1950-2020\n",
    "    df = df[(df['Year'] >= 1950) & (df['Year'] <= 2020)]\n",
    "    \n",
    "    # Get the maximum intensity for each storm\n",
    "    max_intensities = df.groupby('Storm_ID')['Storm_Intensity'].max()\n",
    "    df['Category'] = df['Storm_ID'].map(lambda x: categorize_hurricane(max_intensities[x]))\n",
    "    \n",
    "    # Add AMO phase\n",
    "    df['AMO_Phase'] = df['AMO_Anomaly'].apply(lambda x: 'Warm phase' if x > 0 else 'Cold phase')\n",
    "    \n",
    "    # Generate phase comparison plot\n",
    "    plot_phase_comparison(df)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df75ca15-3038-43f5-955a-efbf72673e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from datetime import datetime\n",
    "\n",
    "def categorize_hurricane(max_wind_speed):\n",
    "    \"\"\"Classify hurricanes based on maximum wind speed\"\"\"\n",
    "    if 34 <= max_wind_speed < 64:\n",
    "        return \"Tropical Storm\"\n",
    "    elif max_wind_speed < 83:\n",
    "        return \"Category 1\"\n",
    "    elif max_wind_speed < 96:\n",
    "        return \"Category 2\"\n",
    "    elif max_wind_speed < 113:\n",
    "        return \"Category 3\"\n",
    "    elif max_wind_speed < 137:\n",
    "        return \"Category 4\"\n",
    "    else:\n",
    "        return \"Category 5\"\n",
    "\n",
    "# Read and process data\n",
    "df = pd.read_csv('merged_amo.csv')\n",
    "\n",
    "# Limit time range to 1950-2020\n",
    "df['Date'] = pd.to_datetime(df['ISO_Time'])\n",
    "df['Year'] = df['Date'].dt.year\n",
    "df = df[(df['Year'] >= 1950) & (df['Year'] <= 2020)]\n",
    "\n",
    "# Calculate maximum intensity for each storm\n",
    "max_intensities = df.groupby('Storm_ID')['Storm_Intensity'].max()\n",
    "unique_storms = df.drop_duplicates('Storm_ID').copy()\n",
    "unique_storms['Max_Intensity'] = unique_storms['Storm_ID'].map(max_intensities)\n",
    "unique_storms['Category'] = unique_storms['Storm_ID'].map(lambda x: categorize_hurricane(max_intensities[x]))\n",
    "unique_storms['AMO_Phase'] = unique_storms['AMO_Anomaly'].apply(lambda x: 'Warm phase' if x > 0 else 'Cold phase')\n",
    "\n",
    "# Create yearly statistics\n",
    "yearly_data = pd.DataFrame(index=range(1950, 2021))\n",
    "yearly_data['AMO_Phase'] = unique_storms.groupby('Year')['AMO_Phase'].first()\n",
    "\n",
    "# Calculate the number of years for each phase\n",
    "phase_years = yearly_data['AMO_Phase'].value_counts()\n",
    "\n",
    "# Create figure\n",
    "fig = plt.figure(figsize=(20, 15))\n",
    "\n",
    "# Define colors\n",
    "category_colors = {\n",
    "    \"Tropical Storm\": (144/255, 238/255, 144/255),    # Light green\n",
    "    \"Category 1\": (238/255, 232/255, 170/255),        # Light yellow\n",
    "    \"Category 2\": (244/255, 164/255, 96/255),         # Light orange\n",
    "    \"Category 3\": (233/255, 150/255, 122/255),        # Light red\n",
    "    \"Category 4\": (221/255, 160/255, 221/255),        # Light purple\n",
    "    \"Category 5\": (147/255, 112/255, 219/255)         # Dark purple\n",
    "}\n",
    "\n",
    "phase_colors = ['#99CCFF', '#FF9999']  # Colors for cold and warm phases\n",
    "\n",
    "# 1. Category distribution\n",
    "plt.subplot(2, 2, 1)\n",
    "category_order = [\"Category 5\", \"Category 4\", \"Category 3\", \n",
    "                 \"Category 2\", \"Category 1\", \"Tropical Storm\"]\n",
    "cat_counts = pd.crosstab(unique_storms['AMO_Phase'], unique_storms['Category'])\n",
    "\n",
    "# Calculate the average number of storms per year for each phase\n",
    "for phase in cat_counts.index:\n",
    "    cat_counts.loc[phase] = cat_counts.loc[phase] / phase_years[phase]\n",
    "\n",
    "cat_counts = cat_counts[category_order]\n",
    "\n",
    "ax = cat_counts.plot(kind='bar', ax=plt.gca(), \n",
    "                    color=[category_colors[cat] for cat in category_order])\n",
    "plt.title('Average Annual Hurricane Category Distribution by AMO Phase\\n(1950-2020)', \n",
    "         fontsize=14, pad=20)\n",
    "plt.xlabel('AMO Phase', fontsize=12)\n",
    "plt.ylabel('Average Number of Storms per Year', fontsize=12)\n",
    "plt.legend(title='Hurricane Category', bbox_to_anchor=(1.05, 1))\n",
    "plt.xticks(rotation=0)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Monthly distribution\n",
    "plt.subplot(2, 2, 2)\n",
    "unique_storms['Month'] = unique_storms['Date'].dt.month\n",
    "monthly_counts = pd.crosstab(unique_storms['AMO_Phase'], unique_storms['Month'])\n",
    "\n",
    "# Ensure all months are present; if missing, fill with 0\n",
    "for month in range(1, 13):\n",
    "    if month not in monthly_counts.columns:\n",
    "        monthly_counts[month] = 0\n",
    "monthly_counts = monthly_counts.reindex(columns=range(1, 13)).fillna(0)\n",
    "\n",
    "# Calculate average monthly distribution for each phase\n",
    "for phase in monthly_counts.index:\n",
    "    monthly_counts.loc[phase] = monthly_counts.loc[phase] / phase_years[phase]\n",
    "\n",
    "month_names = [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \n",
    "              \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
    "\n",
    "for i, phase in enumerate(monthly_counts.index):\n",
    "    plt.plot(range(len(month_names)), monthly_counts.loc[phase], \n",
    "            marker='o', label=phase, color=phase_colors[i], linewidth=2)\n",
    "plt.title('Average Monthly Hurricane Distribution by AMO Phase\\n(1950-2020)', \n",
    "         fontsize=14, pad=20)\n",
    "plt.xlabel('Month', fontsize=12)\n",
    "plt.ylabel('Average Number of Storms per Year', fontsize=12)\n",
    "plt.legend()\n",
    "plt.xticks(range(len(month_names)), month_names, rotation=45)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Maximum intensity distribution (Violin plot)\n",
    "plt.subplot(2, 2, 3)\n",
    "sns.violinplot(data=unique_storms, x='AMO_Phase', y='Max_Intensity', \n",
    "               palette=phase_colors)\n",
    "plt.title('Maximum Storm Intensity Distribution by AMO Phase\\n(1950-2020)', \n",
    "         fontsize=14, pad=20)\n",
    "plt.xlabel('AMO Phase', fontsize=12)\n",
    "plt.ylabel('Maximum Storm Intensity (knots)', fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Relationship between maximum intensity and AMO anomaly\n",
    "plt.subplot(2, 2, 4)\n",
    "\n",
    "# Create color gradient\n",
    "plt.scatter(unique_storms['AMO_Anomaly'], unique_storms['Max_Intensity'], \n",
    "           c=unique_storms['AMO_Anomaly'],\n",
    "           cmap='RdBu_r',\n",
    "           alpha=0.6,\n",
    "           s=30)\n",
    "\n",
    "# Add regression line\n",
    "z = np.polyfit(unique_storms['AMO_Anomaly'], unique_storms['Max_Intensity'], 1)\n",
    "p = np.poly1d(z)\n",
    "x_range = np.linspace(unique_storms['AMO_Anomaly'].min(), \n",
    "                     unique_storms['AMO_Anomaly'].max(), 100)\n",
    "plt.plot(x_range, p(x_range), color='black', linestyle='--', alpha=0.5)\n",
    "\n",
    "# Add correlation coefficient\n",
    "correlation = np.corrcoef(unique_storms['AMO_Anomaly'], unique_storms['Max_Intensity'])[0,1]\n",
    "plt.text(0.05, 0.95, f'Correlation: {correlation:.3f}', \n",
    "         transform=plt.gca().transAxes, fontsize=10)\n",
    "\n",
    "plt.title('Maximum Storm Intensity vs AMO Anomaly\\n(1950-2020)', \n",
    "         fontsize=14, pad=20)\n",
    "plt.xlabel('AMO Anomaly', fontsize=12)\n",
    "plt.ylabel('Maximum Storm Intensity (knots)', fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print statistical summary\n",
    "print(\"\\nStatistical Summary (1950-2020):\")\n",
    "print(f\"\\nAnalysis period: 1950-2020 ({len(yearly_data)} years)\")\n",
    "print(f\"- Warm phase: {phase_years['Warm phase']} years\")\n",
    "print(f\"- Cold phase: {phase_years['Cold phase']} years\")\n",
    "print(\"\\nTotal number of storms:\")\n",
    "print(unique_storms['AMO_Phase'].value_counts())\n",
    "print(\"\\nAverage number of storms per year:\")\n",
    "for phase in phase_years.index:\n",
    "    storms_in_phase = len(unique_storms[unique_storms['AMO_Phase'] == phase])\n",
    "    avg_storms = storms_in_phase / phase_years[phase]\n",
    "    print(f\"{phase}: {avg_storms:.2f}\")\n",
    "    \n",
    "\n",
    "print(\"\\n1. Average Annual Hurricane Count by Intensity Category:\")\n",
    "print(\"\\nStrong hurricanes (Categories 3-5):\")\n",
    "strong_storms = cat_counts[['Category 5', 'Category 4', 'Category 3']]\n",
    "print(strong_storms.sum(axis=1))\n",
    "\n",
    "print(\"\\nModerate hurricanes (Categories 1-2):\")\n",
    "medium_storms = cat_counts[['Category 2', 'Category 1']]\n",
    "print(medium_storms.sum(axis=1))\n",
    "\n",
    "print(\"\\nTropical Storms:\")\n",
    "ts_storms = cat_counts['Tropical Storm']\n",
    "print(ts_storms)\n",
    "\n",
    "print(\"\\n2. Average Maximum Intensity:\")\n",
    "print(unique_storms.groupby('AMO_Phase')['Max_Intensity'].agg(['mean', 'std', 'count']))\n",
    "\n",
    "print(\"\\n3. Average Annual Storm Count During Peak Months (Aug-Oct):\")\n",
    "peak_months = monthly_counts[[8,9,10]]\n",
    "print(\"Average monthly values during peak period:\")\n",
    "print(peak_months)\n",
    "print(\"\\nTotal during peak period:\")\n",
    "print(peak_months.sum(axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3781c7a8-c8b4-4d3d-94b4-0efae2ffd08a",
   "metadata": {},
   "source": [
    "### **2.2 AMO Research Conclusions**\n",
    "\n",
    "During the analysis period from **1950 to 2020** (a total of **71 years**), although the **cold phase** ❄️ lasted longer (**41 years, 634 storms**), the **annual average number of storms** was higher during the **warm phase** 🔥 (**30 years**), reaching **16.73 storms per year**, compared to **15.46 storms per year** in the cold phase. This indicates that **storm activity was more concentrated during the warm phase**.\n",
    "\n",
    "#### 1） Frequency of Major Hurricanes (Categories 3–5)\n",
    "- **During the warm phase** 🔥:  \n",
    "  - The annual average number of **major hurricanes (Categories 3–5)** was **3.50**.  \n",
    "- **During the cold phase** ❄️:  \n",
    "  - The annual average number of **major hurricanes (Categories 3–5)** was **only 2.20**.  \n",
    "- **🔎 Conclusion**: **Major hurricanes tend to form more frequently under warm phase conditions.**  \n",
    "\n",
    "---\n",
    "\n",
    "#### 2） Frequency of Moderate Hurricanes (Categories 1–2) and Tropical Storms\n",
    "- **During the cold phase** ❄️:  \n",
    "  - 🌀 **Categories 1–2 hurricanes**: **8.07 per year**.  \n",
    "  - 🌊 **Tropical storms**: **5.20 per year**.  \n",
    "- **During the warm phase** 🔥:  \n",
    "  - 🌀 **Categories 1–2 hurricanes**: **6.33 per year**.  \n",
    "  - 🌊 **Tropical storms**: **6.90 per year**.  \n",
    "- **🔎 Conclusion**: **Moderate-intensity hurricanes were more frequent during the cold phase, while tropical storms were more frequent during the warm phase**, indicating a **significant difference in storm intensity distribution** between the two phases.  \n",
    "\n",
    "---\n",
    "\n",
    "#### 3） Maximum Storm Intensity 💨\n",
    "- **During the warm phase** 🔥:  \n",
    "  - The **average maximum storm intensity** reached **69.05 knots**.  \n",
    "- **During the cold phase** ❄️:  \n",
    "  - The **average maximum storm intensity** was only **58.82 knots**.  \n",
    "- **🔎 Conclusion**: **Storms not only occurred more frequently during the warm phase, but they were also more intense.**  \n",
    "\n",
    "---\n",
    "\n",
    "#### 4） Hurricane Peak Season (August–October) ⏳\n",
    "- **During the warm phase** 🔥:  \n",
    "  - The total number of storms in the **peak hurricane season (August–October)** was **12.23 per year**, higher than **10.98 per year** in the cold phase.  \n",
    "  - **The difference was particularly pronounced in August** 📈.  \n",
    "- **🔎 Conclusion**: **Storm activity during the peak hurricane season was significantly higher during the warm phase, further amplifying regional storm risk.**  \n",
    "\n",
    "---\n",
    "\n",
    "#### 🏁 Summary 🎯\n",
    "📊 These statistical findings suggest that **the warm phase of the Atlantic Multidecadal Oscillation (AMO) not only leads to a higher annual storm frequency but also promotes the development of more intense hurricanes**, thereby **exerting a greater impact on regional storm risk** ⚠️.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d346e9b-77ba-43b1-80c3-1e22dc75425e",
   "metadata": {},
   "source": [
    "## **3.ENSO Data and Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ec0385-73db-4ccf-8363-dd41ccaf132b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "**3.1 Cleaning and Prepocessing of ENSO and Hurricane data in NA Basin**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44463c5d-b113-457f-ab2d-2829611c5b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load necessary libraries\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.patches as mpatches\n",
    "import IPython.display as display\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Define file paths\n",
    "data_dir = \"../project1\"\n",
    "hurricane_nc_path = os.path.join(data_dir, \"IBTrACS.NA.v04r00.nc\")\n",
    "enso_path = os.path.join(data_dir, \"enso_data.txt\")\n",
    "\n",
    "# Ensure the data directory exists\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)\n",
    "\n",
    "# Download IBTrACS Hurricane Data if not found\n",
    "ibtracs_url = \"https://www.ncei.noaa.gov/data/international-best-track-archive-for-climate-stewardship-ibtracs/v04r00/access/netcdf/IBTrACS.NA.v04r00.nc\"\n",
    "\n",
    "if not os.path.exists(hurricane_nc_path):\n",
    "    print(\"Downloading IBTrACS hurricane data...\")\n",
    "    try:\n",
    "        response = requests.get(ibtracs_url, timeout=60)\n",
    "        response.raise_for_status()  # Raise an error if the request failed\n",
    "        with open(hurricane_nc_path, \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "        print(\"IBTrACS file downloaded successfully.\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Request failed: {e}\")\n",
    "\n",
    "# Load NetCDF hurricane data\n",
    "try:\n",
    "    ds = xr.open_dataset(hurricane_nc_path)\n",
    "except Exception as e:\n",
    "    print(f\"Request failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee75b832-9934-40fc-9a47-7edbce3bdbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert NetCDF dataset to Pandas DataFrame\n",
    "df_hurricane = ds.to_dataframe().reset_index()\n",
    "\n",
    "# Select relevant columns based on dataset structure\n",
    "df_hurricane = df_hurricane.loc[:, [\"sid\", \"season\", \"time\", \"lat\", \"lon\", \"usa_wind\"]].copy()\n",
    "\n",
    "# Rename columns for clarity\n",
    "df_hurricane.columns = [\"Storm_ID\", \"Year\", \"ISO_Time\", \"Latitude\", \"Longitude\", \"Storm_Intensity\"]\n",
    "\n",
    "# Decode `sid` from bytes to string\n",
    "df_hurricane[\"Storm_ID\"] = df_hurricane[\"Storm_ID\"].str.decode(\"utf-8\")\n",
    "\n",
    "# Convert Year to integer (from float)\n",
    "df_hurricane[\"Year\"] = df_hurricane[\"Year\"].astype(int)\n",
    "\n",
    "# Filter years from 1950 onwards\n",
    "df_hurricane = df_hurricane[df_hurricane[\"Year\"] >= 1950]\n",
    "\n",
    "# Convert `ISO_Time` to datetime format\n",
    "df_hurricane[\"ISO_Time\"] = pd.to_datetime(df_hurricane[\"ISO_Time\"], errors=\"coerce\")\n",
    "\n",
    "# Drop rows where conversion failed\n",
    "df_hurricane = df_hurricane.dropna(subset=[\"ISO_Time\"])\n",
    "\n",
    "# Extract Month safely\n",
    "df_hurricane[\"Month\"] = df_hurricane[\"ISO_Time\"].dt.month.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4c12d7-5083-479e-aa89-2c5f1322944f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Month from number to text for merging with ENSO\n",
    "month_map = {\n",
    "    1: \"Jan\", 2: \"Feb\", 3: \"Mar\", 4: \"Apr\", 5: \"May\", 6: \"Jun\",\n",
    "    7: \"Jul\", 8: \"Aug\", 9: \"Sep\", 10: \"Oct\", 11: \"Nov\", 12: \"Dec\"\n",
    "}\n",
    "df_hurricane[\"Month\"] = df_hurricane[\"Month\"].map(month_map)\n",
    "\n",
    "# Define hurricane category mapping (based on wind speed in knots)\n",
    "def categorize_hurricane(wind_speed):\n",
    "    if wind_speed < 64:\n",
    "        return \"Tropical Storm\"\n",
    "    elif 64 <= wind_speed <= 82:\n",
    "        return \"Category 1\"\n",
    "    elif 83 <= wind_speed <= 95:\n",
    "        return \"Category 2\"\n",
    "    elif 96 <= wind_speed <= 112:\n",
    "        return \"Category 3\"\n",
    "    elif 113 <= wind_speed <= 136:\n",
    "        return \"Category 4\"\n",
    "    else:\n",
    "        return \"Category 5\"\n",
    "\n",
    "# Apply category mapping\n",
    "df_hurricane[\"Category\"] = df_hurricane[\"Storm_Intensity\"].apply(categorize_hurricane)\n",
    "\n",
    "# Save the cleaned dataset\n",
    "df_hurricane.to_csv(\"../project1/hurricane_cleaned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2cd53a-da89-4617-bd76-8a083cadc0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning ENSO table\n",
    "\n",
    "# Load ENSO data\n",
    "df_enso = pd.read_csv(enso_path, sep=r'\\s+', header=None)\n",
    "\n",
    "# Rename columns\n",
    "df_enso.columns = [\"Year\", \"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
    "\n",
    "# Remove non-numeric rows (e.g., headers, footers)\n",
    "df_enso = df_enso[df_enso[\"Year\"].astype(str).str.match(r'^\\d{4}$')]\n",
    "\n",
    "# Convert Year to integer\n",
    "df_enso[\"Year\"] = df_enso[\"Year\"].astype(int)\n",
    "\n",
    "# Convert to long format\n",
    "df_enso_long = df_enso.melt(id_vars=[\"Year\"], var_name=\"Month\", value_name=\"SST_Anomaly\")\n",
    "\n",
    "# Convert SST_Anomaly to numeric\n",
    "df_enso_long[\"SST_Anomaly\"] = pd.to_numeric(df_enso_long[\"SST_Anomaly\"], errors=\"coerce\")\n",
    "\n",
    "# Sort values (important for applying 5-month rule)\n",
    "df_enso_long = df_enso_long.sort_values(by=[\"Year\", \"Month\"]).reset_index(drop=True)\n",
    "\n",
    "# Initialize ENSO phase column\n",
    "df_enso_long[\"ENSO_Phase\"] = \"Neutral\"\n",
    "\n",
    "# Function to apply 5-month rule\n",
    "def detect_enso_events(df):\n",
    "    consecutive_count = 0\n",
    "    current_phase = \"Neutral\"\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        anomaly = df.loc[i, \"SST_Anomaly\"]\n",
    "\n",
    "        # Determine the phase for this month\n",
    "        if anomaly >= 0.5:\n",
    "            phase = \"El Niño\"\n",
    "        elif anomaly <= -0.5:\n",
    "            phase = \"La Niña\"\n",
    "        else:\n",
    "            phase = \"Neutral\"\n",
    "\n",
    "        # If we are continuing the same phase, increase count\n",
    "        if phase == current_phase:\n",
    "            consecutive_count += 1\n",
    "        else:\n",
    "            # If we switch phases, reset the counter\n",
    "            consecutive_count = 1\n",
    "            current_phase = phase\n",
    "\n",
    "        # If phase has lasted for at least 5 months, apply to previous months\n",
    "        if consecutive_count >= 5:\n",
    "            for j in range(i - 4, i + 1):\n",
    "                df.loc[j, \"ENSO_Phase\"] = current_phase  # Apply to previous 5 months\n",
    "\n",
    "    return df\n",
    "\n",
    "# Apply 5-month rule\n",
    "df_enso_long = detect_enso_events(df_enso_long)\n",
    "df_enso_long.to_csv(\"../project1/enso_cleaned.csv\", index=False)\n",
    "df_enso_long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6e0712-ec15-4ec3-bca1-5e0991c16c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge ENSO and hurricane data together\n",
    "df_hurricane[\"Year\"] = df_hurricane[\"Year\"].astype(int)\n",
    "df_hurricane[\"Month\"] = df_hurricane[\"Month\"].astype(str)\n",
    "\n",
    "df_enso_long[\"Year\"] = df_enso_long[\"Year\"].astype(int)\n",
    "df_enso_long[\"Month\"] = df_enso_long[\"Month\"].astype(str)\n",
    "df_enso_long = df_enso_long[df_enso_long[\"Year\"] >= 1950]\n",
    "\n",
    "# Define a dictionary to map abbreviated month names to full names\n",
    "month_map = {\n",
    "    \"Jan\": \"January\", \"Feb\": \"February\", \"Mar\": \"March\", \"Apr\": \"April\", \"May\": \"May\", \"Jun\": \"June\",\n",
    "    \"Jul\": \"July\", \"Aug\": \"August\", \"Sep\": \"September\", \"Oct\": \"October\", \"Nov\": \"November\", \"Dec\": \"December\"\n",
    "}\n",
    "\n",
    "# Apply the mapping\n",
    "df_hurricane[\"Month\"] = df_hurricane[\"Month\"].map(month_map)\n",
    "df_enso_long.loc[:, \"Month\"] = df_enso_long[\"Month\"].map(month_map)\n",
    "\n",
    "# Ensure all Month values are consistent by stripping spaces\n",
    "df_hurricane[\"Month\"] = df_hurricane[\"Month\"].str.strip()\n",
    "df_enso_long.loc[:, \"Month\"] = df_enso_long[\"Month\"].str.strip()\n",
    "\n",
    "# Convert to lowercase (optional but helps prevent mismatches)\n",
    "df_hurricane[\"Month\"] = df_hurricane[\"Month\"].str.capitalize()\n",
    "df_enso_long.loc[:, \"Month\"] = df_enso_long[\"Month\"].str.capitalize()\n",
    "df_hurricane = df_hurricane[df_hurricane[\"Year\"] >= 1950]\n",
    "\n",
    "# Merge hurricane data with ENSO data on Year & Month\n",
    "df_merged_ENSO = df_hurricane.merge(df_enso_long, on=[\"Year\", \"Month\"], how=\"left\")\n",
    "\n",
    "# Save merged dataset\n",
    "df_merged_ENSO.to_csv(\"../project1/merged_enso.csv\", index=False)\n",
    "\n",
    "df_merged_ENSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9179c618-c76c-4118-96db-841cf3f716e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate by Year (Average SST anomaly per year)\n",
    "df_yearly = df_enso_long.groupby(\"Year\")[\"SST_Anomaly\"].mean().reset_index()\n",
    "\n",
    "# Classify ENSO Phase for yearly data\n",
    "def classify_enso(value):\n",
    "    if value >= 0.5:\n",
    "        return \"El Niño\"\n",
    "    elif value <= -0.5:\n",
    "        return \"La Niña\"\n",
    "    else:\n",
    "        return \"Neutral\"\n",
    "\n",
    "df_yearly[\"ENSO_Phase\"] = df_yearly[\"SST_Anomaly\"].apply(classify_enso)\n",
    "\n",
    "# Set figure size\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot positive bars for El Niño (Red)\n",
    "plt.bar(df_yearly[\"Year\"], df_yearly[\"SST_Anomaly\"], color=df_yearly[\"SST_Anomaly\"].apply(lambda x: \"red\" if x >= 0.5 else \"blue\" if x <= -0.5 else \"gray\"), width=1.0)\n",
    "\n",
    "# Add a horizontal line at y=0\n",
    "plt.axhline(0, color=\"black\", linewidth=1)\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"MEI (Multivariate ENSO Index)\")\n",
    "plt.title(\"Multivariate ENSO Index (MEI) - ENSO Variability Over Time\")\n",
    "\n",
    "# Add a legend for ENSO phases\n",
    "legend_labels = [\n",
    "    mpatches.Patch(color=\"red\", label=\"El Niño (≥ 0.5)\"),\n",
    "    mpatches.Patch(color=\"blue\", label=\"La Niña (≤ -0.5)\"),\n",
    "    mpatches.Patch(color=\"gray\", label=\"Neutral (-0.5 to 0.5)\")\n",
    "]\n",
    "plt.legend(handles=legend_labels, loc=\"upper left\")\n",
    "\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d9dab2-1492-460f-ba3c-7ec15d4bf7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ENSO visualization over years\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Aggregate by Month (Average SST anomaly per month across years)\n",
    "df_monthly = df_enso_long.groupby([\"Year\", \"Month\"])[\"SST_Anomaly\"].mean().reset_index()\n",
    "\n",
    "# Set up figure\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Define ENSO colors\n",
    "enso_colors = df_monthly[\"SST_Anomaly\"].apply(lambda x: \"red\" if x >= 0.5 else \"blue\" if x <= -0.5 else \"gray\")\n",
    "\n",
    "# Create bar plot for monthly SST anomalies\n",
    "plt.bar(df_monthly.index, df_monthly[\"SST_Anomaly\"], color=enso_colors, width=1.0, alpha=0.7)\n",
    "\n",
    "# Add smoothed 5-year rolling mean to show ENSO variability over time\n",
    "df_monthly[\"Rolling_MEI\"] = df_monthly[\"SST_Anomaly\"].rolling(60, min_periods=1).mean()\n",
    "plt.plot(df_monthly.index, df_monthly[\"Rolling_MEI\"], color=\"black\", linewidth=1.5, label=\"Smoothed ENSO Index (5-Year Rolling Mean)\")\n",
    "\n",
    "# Add zero reference line\n",
    "plt.axhline(0, color=\"black\", linewidth=1)\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel(\"Time (1950-2023)\", fontsize=12)\n",
    "plt.ylabel(\"SST Anomaly (°C)\", fontsize=12)\n",
    "plt.title(\"ENSO Variability Over Time (Monthly SST Anomalies)\", fontsize=14, fontweight=\"bold\")\n",
    "\n",
    "# Format x-axis\n",
    "plt.xticks(np.arange(0, len(df_monthly), step=120), df_monthly[\"Year\"].iloc[::120].astype(int), rotation=45)  # Show every 10 years\n",
    "plt.ylim(-2.5, 3)\n",
    "\n",
    "# Add grid for readability\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "# Add legend\n",
    "legend_labels = [\n",
    "    plt.Line2D([0], [0], color=\"red\", linewidth=4, label=\"El Niño (≥ 0.5)\"),\n",
    "    plt.Line2D([0], [0], color=\"blue\", linewidth=4, label=\"La Niña (≤ -0.5)\"),\n",
    "    plt.Line2D([0], [0], color=\"gray\", linewidth=4, label=\"Neutral (-0.5 to 0.5)\"),\n",
    "    plt.Line2D([0], [0], color=\"black\", linewidth=1.5, label=\"Smoothed ENSO Index\")\n",
    "]\n",
    "plt.legend(handles=legend_labels, loc=\"upper left\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a304ef8-ce94-4fcd-b018-5bb0802ffe49",
   "metadata": {},
   "outputs": [],
   "source": [
    "Visualize ENSO tracks over the years\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Function to classify hurricanes based on max wind speed\n",
    "def categorize_hurricane(max_wind_speed):\n",
    "    \"\"\"Classify hurricanes based on maximum wind speed\"\"\"\n",
    "    if 34 <= max_wind_speed < 64:\n",
    "        return \"Tropical Storm\"\n",
    "    elif max_wind_speed < 83:\n",
    "        return \"Category 1\"\n",
    "    elif max_wind_speed < 96:\n",
    "        return \"Category 2\"\n",
    "    elif max_wind_speed < 113:\n",
    "        return \"Category 3\"\n",
    "    elif max_wind_speed < 137:\n",
    "        return \"Category 4\"\n",
    "    else:\n",
    "        return \"Category 5\"\n",
    "\n",
    "# Function to plot ENSO phase comparison\n",
    "def plot_enso_comparison(df, save_path=None):\n",
    "    \"\"\"\n",
    "    Plot hurricane tracks for different ENSO phases (El Niño, La Niña).\n",
    "    \"\"\"\n",
    "    # Create a 6x2 grid of subplots\n",
    "    fig = plt.figure(figsize=(20, 30))\n",
    "\n",
    "    # Define categories and colors\n",
    "    category_colors = {\n",
    "        \"Tropical Storm\": (144/255, 238/255, 144/255),    # Light green\n",
    "        \"Category 1\": (238/255, 232/255, 170/255),        # Light yellow\n",
    "        \"Category 2\": (244/255, 164/255, 96/255),         # Light orange\n",
    "        \"Category 3\": (233/255, 150/255, 122/255),        # Light red\n",
    "        \"Category 4\": (221/255, 160/255, 221/255),        # Light purple\n",
    "        \"Category 5\": (147/255, 112/255, 219/255)         # Dark purple\n",
    "    }\n",
    "\n",
    "    categories = [\"Category 5\", \"Category 4\", \"Category 3\", \n",
    "                  \"Category 2\", \"Category 1\", \"Tropical Storm\"]\n",
    "    phases = [\"El Niño\", \"La Niña\"]\n",
    "\n",
    "    for cat_idx, category in enumerate(categories):\n",
    "        for phase_idx, phase in enumerate(phases):\n",
    "            ax = plt.subplot(6, 2, cat_idx * 2 + phase_idx + 1, projection=ccrs.PlateCarree())\n",
    "\n",
    "            # Set map extent\n",
    "            ax.set_extent([-100, 0, 0, 60], crs=ccrs.PlateCarree())\n",
    "\n",
    "            # Add natural geographic features\n",
    "            ax.add_feature(cfeature.LAND, edgecolor='black', facecolor=\"beige\")\n",
    "            ax.add_feature(cfeature.OCEAN, facecolor=cfeature.COLORS['water'])\n",
    "            ax.add_feature(cfeature.BORDERS, linestyle=':')\n",
    "            ax.add_feature(cfeature.COASTLINE)\n",
    "\n",
    "            # Add gridlines\n",
    "            gl = ax.gridlines(draw_labels=True, linewidth=0.5, color='gray', alpha=0.5, linestyle='--')\n",
    "            gl.top_labels = False\n",
    "            gl.right_labels = False if phase_idx == 0 else False\n",
    "            gl.left_labels = True if phase_idx == 0 else False\n",
    "\n",
    "            # Filter data\n",
    "            storm_ids = df[df['Category'] == category]['Storm_ID'].unique()\n",
    "            storm_data = df[df['Storm_ID'].isin(storm_ids) & (df['ENSO_Phase'] == phase)]\n",
    "\n",
    "            if len(storm_data) > 0:\n",
    "                plotted_storms = set()\n",
    "                for storm_id in storm_data['Storm_ID'].unique():\n",
    "                    if storm_id in plotted_storms:\n",
    "                        continue\n",
    "\n",
    "                    track = df[df['Storm_ID'] == storm_id].sort_values('ISO_Time')\n",
    "                    track = track.iloc[::16, :]  # Reduce point density\n",
    "\n",
    "                    # Plot track line\n",
    "                    ax.plot(track['Longitude'], track['Latitude'],\n",
    "                            color='black',\n",
    "                            linewidth=0.8,\n",
    "                            alpha=0.4,\n",
    "                            zorder=2)\n",
    "\n",
    "                    # Add points\n",
    "                    ax.scatter(track['Longitude'], track['Latitude'],\n",
    "                               color=category_colors[category],\n",
    "                               s=15,\n",
    "                               alpha=0.6,\n",
    "                               edgecolor='black',\n",
    "                               linewidth=0.5,\n",
    "                               zorder=3)\n",
    "\n",
    "                    plotted_storms.add(storm_id)\n",
    "\n",
    "            # Add subplot title\n",
    "            storm_count = len(set(storm_data['Storm_ID']))\n",
    "            ax.set_title(f\"{category} - {phase}\\n({storm_count} storms)\", fontsize=12, pad=10)\n",
    "\n",
    "    # Add overall title\n",
    "    plt.suptitle(\"1950-2023 North Atlantic Hurricanes by ENSO Phase\", fontsize=16, y=0.92)\n",
    "\n",
    "    # Adjust subplot spacing\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    # Read data\n",
    "    df = pd.read_csv('merged_enso.csv')\n",
    "\n",
    "    # Convert date and extract year\n",
    "    df['Year'] = pd.to_datetime(df['ISO_Time']).dt.year\n",
    "\n",
    "    # Restrict time range to 1950-2023\n",
    "    df = df[(df['Year'] >= 1950) & (df['Year'] <= 2023)]\n",
    "\n",
    "    # Get maximum intensity for each storm\n",
    "    max_intensities = df.groupby('Storm_ID')['Storm_Intensity'].max()\n",
    "    df['Category'] = df['Storm_ID'].map(lambda x: categorize_hurricane(max_intensities[x]))\n",
    "\n",
    "    # Assign ENSO phase based on SST anomaly\n",
    "    df['ENSO_Phase'] = df['SST_Anomaly'].apply(lambda x: 'El Niño' if x > 0.5 else 'La Niña' if x < -0.5 else 'Neutral')\n",
    "\n",
    "    # Generate phase comparison plot\n",
    "    plot_enso_comparison(df)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e24a10c-4ab7-48b4-9a86-14f8c73b1cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Frequency of hurricanes during different ENSO stages\n",
    "\n",
    "# Ensure ENSO Phase is categorized correctly\n",
    "df_merged_ENSO[\"ENSO_Phase\"] = df_merged_ENSO[\"ENSO_Phase\"].astype(str)\n",
    "\n",
    "# Total length (years) per ENSO Phase\n",
    "total_years = df_merged_ENSO.groupby(\"ENSO_Phase\")[\"Year\"].nunique()\n",
    "\n",
    "# Number of unique hurricanes per ENSO Phase\n",
    "num_hurricanes = df_merged_ENSO.groupby(\"ENSO_Phase\")[\"Storm_ID\"].nunique()\n",
    "\n",
    "# Hurricanes per year per ENSO Phase\n",
    "hurricanes_per_year = num_hurricanes / total_years\n",
    "\n",
    "# Combine into a DataFrame\n",
    "enso_summary = pd.DataFrame({\n",
    "    \"Total Length (years)\": total_years,\n",
    "    \"# Of Hurricanes\": num_hurricanes,\n",
    "    \"Hurricanes/Year\": hurricanes_per_year\n",
    "})\n",
    "\n",
    "# Reset index for better readability\n",
    "enso_summary = enso_summary.reset_index()\n",
    "\n",
    "# Display the summary table\n",
    "fig, ax = plt.subplots(figsize=(12, 2))\n",
    "ax.axis('tight')\n",
    "ax.axis('off')\n",
    "\n",
    "table = ax.table(cellText=enso_summary.values, colLabels=enso_summary.columns, cellLoc='center', loc='center')\n",
    "\n",
    "# Formatting the header row\n",
    "for key, cell in table.get_celld().items():\n",
    "    if key[0] == 0:  # Header row\n",
    "        cell.set_text_props(weight='bold')\n",
    "\n",
    "# Show the table\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0541f99-bbbe-439b-96fa-a977ca8b0107",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Function to classify hurricanes based on wind speed\n",
    "def categorize_hurricane(max_wind_speed):\n",
    "    \"\"\"Classify hurricanes based on maximum wind speed\"\"\"\n",
    "    if 34 <= max_wind_speed < 64:\n",
    "        return \"Tropical Storm\"\n",
    "    elif max_wind_speed < 83:\n",
    "        return \"Category 1\"\n",
    "    elif max_wind_speed < 96:\n",
    "        return \"Category 2\"\n",
    "    elif max_wind_speed < 113:\n",
    "        return \"Category 3\"\n",
    "    elif max_wind_speed < 137:\n",
    "        return \"Category 4\"\n",
    "    else:\n",
    "        return \"Category 5\"\n",
    "\n",
    "# Read and process the ENSO dataset\n",
    "df = df_merged_ENSO.copy()\n",
    "\n",
    "# Convert date and limit time range to 1950-2020\n",
    "df['Date'] = pd.to_datetime(df['ISO_Time'])\n",
    "df['Year'] = df['Date'].dt.year\n",
    "df = df[(df['Year'] >= 1950) & (df['Year'] <= 2020)]\n",
    "\n",
    "# Get maximum hurricane intensity for each storm\n",
    "max_intensities = df.groupby('Storm_ID')['Storm_Intensity'].max()\n",
    "unique_storms = df.drop_duplicates('Storm_ID').copy()\n",
    "unique_storms['Max_Intensity'] = unique_storms['Storm_ID'].map(max_intensities)\n",
    "\n",
    "# Categorize hurricanes by intensity\n",
    "unique_storms['Category'] = unique_storms['Storm_ID'].map(lambda x: categorize_hurricane(max_intensities[x]))\n",
    "\n",
    "# Define ENSO phases\n",
    "unique_storms['ENSO_Phase'] = unique_storms['SST_Anomaly'].apply(\n",
    "    lambda x: 'El Niño' if x > 0.5 else 'La Niña' if x < -0.5 else 'Neutral'\n",
    ")\n",
    "\n",
    "# Create yearly statistics\n",
    "yearly_data = pd.DataFrame(index=range(1950, 2021))\n",
    "yearly_data['ENSO_Phase'] = unique_storms.groupby('Year')['ENSO_Phase'].first()\n",
    "\n",
    "# Count number of years in each phase\n",
    "phase_years = yearly_data['ENSO_Phase'].value_counts()\n",
    "\n",
    "# Create figure\n",
    "fig = plt.figure(figsize=(20, 15))\n",
    "\n",
    "# Define colors for hurricane categories\n",
    "category_colors = {\n",
    "    \"Tropical Storm\": \"gray\",\n",
    "    \"Category 1\": \"yellow\",\n",
    "    \"Category 2\": \"orange\",\n",
    "    \"Category 3\": \"red\",\n",
    "    \"Category 4\": \"purple\",\n",
    "    \"Category 5\": \"black\"\n",
    "}\n",
    "\n",
    "# Colors for ENSO phases (dynamically mapped)\n",
    "phase_colors_map = {'El Niño': '#FF9999', 'La Niña': '#99CCFF', 'Neutral': '#CCCCCC'}\n",
    "existing_phases = list(unique_storms['ENSO_Phase'].unique())  # Get actual phases in data\n",
    "phase_colors = [phase_colors_map[phase] for phase in existing_phases]\n",
    "\n",
    "### **1️⃣ Category Distribution by ENSO Phase**\n",
    "plt.subplot(2, 2, 1)\n",
    "category_order = [\"Category 5\", \"Category 4\", \"Category 3\", \"Category 2\", \"Category 1\", \"Tropical Storm\"]\n",
    "cat_counts = pd.crosstab(unique_storms['ENSO_Phase'], unique_storms['Category'])\n",
    "\n",
    "# Ensure cat_counts allows float values\n",
    "cat_counts = cat_counts.astype(float)\n",
    "\n",
    "# Normalize by years in each phase\n",
    "for phase in cat_counts.index:\n",
    "    cat_counts.loc[phase] = (cat_counts.loc[phase] / phase_years[phase]).astype(float)\n",
    "\n",
    "cat_counts = cat_counts[category_order]\n",
    "\n",
    "ax = cat_counts.plot(kind='bar', ax=plt.gca(), \n",
    "                     color=[category_colors[cat] for cat in category_order])\n",
    "plt.title('Average Annual Hurricane Category Distribution by ENSO Phase\\n(1950-2020)', fontsize=14, pad=20)\n",
    "plt.xlabel('ENSO Phase', fontsize=12)\n",
    "plt.ylabel('Average Number of Storms per Year', fontsize=12)\n",
    "plt.legend(title='Hurricane Category', bbox_to_anchor=(1.05, 1))\n",
    "plt.xticks(rotation=0)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "### **2️⃣ Monthly Hurricane Distribution by ENSO Phase**\n",
    "plt.subplot(2, 2, 2)\n",
    "unique_storms['Month'] = unique_storms['Date'].dt.month\n",
    "monthly_counts = pd.crosstab(unique_storms['ENSO_Phase'], unique_storms['Month'])\n",
    "\n",
    "# Ensure all months are present\n",
    "for month in range(1, 13):\n",
    "    if month not in monthly_counts.columns:\n",
    "        monthly_counts[month] = 0\n",
    "monthly_counts = monthly_counts.reindex(columns=range(1, 13)).fillna(0)\n",
    "monthly_counts = monthly_counts.astype(float)\n",
    "\n",
    "# Normalize by years in each ENSO phase\n",
    "for phase in monthly_counts.index:\n",
    "    monthly_counts.loc[phase] = monthly_counts.loc[phase].astype(float) / float(phase_years[phase])\n",
    "\n",
    "month_names = [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
    "\n",
    "for i, phase in enumerate(existing_phases):\n",
    "    plt.plot(range(len(month_names)), monthly_counts.loc[phase], \n",
    "             marker='o', label=phase, color=phase_colors[i], linewidth=2)\n",
    "\n",
    "plt.title('Average Monthly Hurricane Distribution by ENSO Phase\\n(1950-2020)', fontsize=14, pad=20)\n",
    "plt.xlabel('Month', fontsize=12)\n",
    "plt.ylabel('Average Number of Storms per Year', fontsize=12)\n",
    "plt.legend()\n",
    "plt.xticks(range(len(month_names)), month_names, rotation=45)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "### **3️⃣ Violin Plot: Max Intensity by ENSO Phase**\n",
    "plt.subplot(2, 2, 3)\n",
    "sns.violinplot(data=unique_storms, x='ENSO_Phase', y='Max_Intensity', hue='ENSO_Phase', \n",
    "               palette=phase_colors, legend=False)\n",
    "plt.title('Maximum Storm Intensity Distribution by ENSO Phase\\n(1950-2020)', fontsize=14, pad=20)\n",
    "plt.xlabel('ENSO Phase', fontsize=12)\n",
    "plt.ylabel('Maximum Storm Intensity (knots)', fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "### **4️⃣ Scatter Plot: Max Intensity vs ENSO Anomaly**\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.scatter(unique_storms['SST_Anomaly'], unique_storms['Max_Intensity'], \n",
    "            c=unique_storms['SST_Anomaly'], cmap='RdBu_r', alpha=0.6, s=30)\n",
    "\n",
    "# Add regression line\n",
    "z = np.polyfit(unique_storms['SST_Anomaly'], unique_storms['Max_Intensity'], 1)\n",
    "p = np.poly1d(z)\n",
    "x_range = np.linspace(unique_storms['SST_Anomaly'].min(), unique_storms['SST_Anomaly'].max(), 100)\n",
    "plt.plot(x_range, p(x_range), color='black', linestyle='--', alpha=0.5)\n",
    "\n",
    "# Add correlation coefficient\n",
    "correlation = np.corrcoef(unique_storms['SST_Anomaly'], unique_storms['Max_Intensity'])[0,1]\n",
    "plt.text(0.05, 0.95, f'Correlation: {correlation:.3f}', \n",
    "         transform=plt.gca().transAxes, fontsize=10)\n",
    "\n",
    "plt.title('Maximum Storm Intensity vs ENSO Anomaly\\n(1950-2020)', fontsize=14, pad=20)\n",
    "plt.xlabel('SST Anomaly', fontsize=12)\n",
    "plt.ylabel('Maximum Storm Intensity (knots)', fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print statistical summary\n",
    "print(\"\\nStatistical Summary (1950-2020):\")\n",
    "print(f\"\\nAnalysis period: 1950-2023 ({len(yearly_data)} years)\")\n",
    "print(\"\\nTotal number of storms per ENSO phase:\")\n",
    "print(unique_storms['ENSO_Phase'].value_counts())\n",
    "\n",
    "print(\"\\nAverage Maximum Intensity per ENSO Phase:\")\n",
    "print(unique_storms.groupby('ENSO_Phase')['Max_Intensity'].agg(['mean', 'std', 'count']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31aaa22a-9cbf-4e33-9b88-4c7bc0b42888",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### **3.2 ENSO Research Conclusion**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780f8371-a473-4762-951e-3786a20dc479",
   "metadata": {},
   "source": [
    "####  ENSO Exhibits Clear Periodicity (Every 2–7 Years):\n",
    "\n",
    "- El Niño and La Niña events alternate, displaying a periodicity consistent with known ENSO patterns.\n",
    "\n",
    "#### Intensification of ENSO Phenomena Post-1980:\n",
    "\n",
    "- Between 1950 and 1980, ENSO variations were relatively mild.\n",
    "- Notably strong El Niño events occurred in 1982–83, 1997–98, and 2015–16, possibly linked to global warming.\n",
    "\n",
    "#### Prolonged Duration of La Niña Events:\n",
    "\n",
    "- Historically, La Niña events lasted 1–2 years; however, the 2020–22 La Niña persisted for 3 years, indicating potential changes in ENSO patterns.\n",
    "\n",
    "#### Potential Impact of ENSO on Hurricane Activity:\n",
    "\n",
    "- During El Niño periods, the number of Atlantic hurricanes tends to decrease.\n",
    "- Conversely, during La Niña periods, Atlantic hurricane activity tends to increase.\n",
    "- Further research is needed to quantify ENSO's influence on hurricane intensity and trajectories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb24fab-c8ab-4921-919a-32b6d227c16f",
   "metadata": {},
   "source": [
    "### **3.3 AMO 🆚 ENSO Impact on Hurricanes**\n",
    "| Factor | AMO Positive | AMO Negative | El Niño | La Niña |\n",
    "|---|---|---|---|---|\n",
    "| **Hurricane Frequency** | Increased | Decreased | Decreased | Increased |\n",
    "| **Hurricane Intensity** | Stronger (More Category 3-5) | Weaker (Mostly Category 1-2) | Weaker | Stronger |\n",
    "| **Hurricane Paths** | More westward, landfall on the U.S. | More eastward, fewer landfalls | More eastward | More westward, stronger landfalls |\n",
    "| **Hurricane Lifespan** | Longer | Shorter | Shorter | Longer |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34217d50-f0a3-4909-978c-a063eec76d3c",
   "metadata": {},
   "source": [
    "#### In general, we found that the AMO has a more significant impact on hurricanes than ENSO, probably because ENSO has a greater impact on the Pacific Ocean.\n",
    "#### So next, we will use K- Means to cluster the hurricane tracks formed by the AMO. Let’s see what new findings we will get.\n",
    "#### Welcome to **Part 4** 🎉"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abb5bff-aa1b-435e-8572-1ad6fc9ebed4",
   "metadata": {},
   "source": [
    "## **4. The Mathematical Portrait of Hurricanes: Higher-Order Moment Analysis & K-Means Clustering**\n",
    "### **4.1 What Are Higher-Order Moments?**\n",
    "Higher-order moments describe the shape of a distribution. For hurricanes, we define:\n",
    "\n",
    "- **1st Moment (Centroid)**: The **average position** of the hurricane track.\n",
    "- **2nd Moment (Variance)**: The **spread** of the hurricane’s trajectory.\n",
    "- **3rd Moment (Skewness)**: The **asymmetry** of the hurricane path.\n",
    "- **4th Moment (Kurtosis)**: The **concentration or dispersion** of the trajectory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be2da47-1753-4f23-bb31-6ccdedde403e",
   "metadata": {},
   "source": [
    "### **4.2 K-Means Code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f033409e-eb2a-42fe-a310-c061ec694a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install kneed\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "from matplotlib.dates import YearLocator, DateFormatter\n",
    "from matplotlib.patches import Ellipse\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from kneed import KneeLocator\n",
    "from scipy.stats import skew, kurtosis\n",
    "from IPython.display import display\n",
    "\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145513b7-3f4f-4c36-9bcb-26cd959fd699",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading AMO data and caculate 10_year_rolling\n",
    "file_path = \"ersst.v5.amo.dat.txt\" \n",
    "AMO_time = pd.read_csv(file_path, sep=r'\\s+', skiprows=2, names=[\"Year\", \"Month\", \"SSTA\"])\n",
    "\n",
    "AMO_time = AMO_time.astype({\"Year\": int, \"Month\": int, \"SSTA\": float})\n",
    "AMO_time[\"Date\"] = AMO_time[\"Year\"].astype(str) + \"-\" + AMO_time[\"Month\"].astype(str).str.zfill(2)\n",
    "AMO_time = AMO_time[[\"Date\", \"SSTA\"]]\n",
    "\n",
    "AMO_time[\"AMO_Smoothed\"] = AMO_time[\"SSTA\"].rolling(window=121, center=True).mean()\n",
    "AMO_time = AMO_time.dropna(subset=[\"AMO_Smoothed\"])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc86798a-353d-45e6-9e8a-ae4d92072a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show AMO plot\n",
    "AMO_time['Date'] = pd.to_datetime(AMO_time['Date'], format='%Y-%m')\n",
    "zero_crossings = np.where(np.diff(np.sign(AMO_time['AMO_Smoothed'])))[0]\n",
    "zero_dates = AMO_time.iloc[zero_crossings]['Date']\n",
    "zero_values = AMO_time.iloc[zero_crossings]['AMO_Smoothed']  # 这些值接近 0\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "ax.fill_between(AMO_time['Date'], AMO_time['AMO_Smoothed'], 0,\n",
    "                where=(AMO_time['AMO_Smoothed'] > 0),\n",
    "                color='red', alpha=0.7, label='Warm Phase')\n",
    "ax.fill_between(AMO_time['Date'], AMO_time['AMO_Smoothed'], 0,\n",
    "                where=(AMO_time['AMO_Smoothed'] <= 0),\n",
    "                color='blue', alpha=0.7, label='Cool Phase')\n",
    "ax.plot(AMO_time['Date'], AMO_time['AMO_Smoothed'], color='black', linewidth=2)\n",
    "\n",
    "for date in zero_dates:\n",
    "    ax.axvline(date, color='gray', linestyle='dashed', alpha=0.6)\n",
    "    ax.text(date, 0.05, date.strftime('%Y-%m'), rotation=90, verticalalignment='bottom', fontsize=10)\n",
    "\n",
    "ax.set_title(\"Atlantic Multi-Decadal Oscillation (AMO) Index\\nSmoothed Trend (10-year Rolling Mean)\", \n",
    "             fontsize=16, fontweight=\"bold\", pad=20)\n",
    "ax.xaxis.set_major_locator(YearLocator(20))  # 每 20 年标一个刻度\n",
    "ax.xaxis.set_major_formatter(DateFormatter(\"%Y\"))\n",
    "ax.set_xlim(AMO_time['Date'].min(), AMO_time['Date'].max())\n",
    "ax.set_ylim(-1.5, 1.5)\n",
    "ax.grid(True, alpha=0.3, linestyle=\"--\")\n",
    "ax.spines[\"top\"].set_visible(False)\n",
    "ax.spines[\"right\"].set_visible(False)\n",
    "ax.legend(loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f66c432-8528-4394-aa86-8b02d14a693b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading hurrican date\n",
    "tks = xr.open_dataset('data/NA_data.nc', engine=\"netcdf4\", decode_times=False)\n",
    "tks_na = tks.where(tks.basin == b'NA', drop=True)\n",
    "time_units = tks_na.time.attrs.get(\"units\", \"days since 1858-11-17\")\n",
    "time_base = pd.to_datetime(time_units.split(\"since\")[-1].strip())\n",
    "time_values = tks_na.time.values\n",
    "time_values_flat = time_values.ravel()\n",
    "time_converted = (pd.to_datetime(time_base) + pd.to_timedelta(time_values_flat, unit=\"D\")).to_numpy().reshape(time_values.shape)\n",
    "tks_na = tks_na.assign_coords(time=((\"storm\", \"date_time\"), time_converted))\n",
    "# filter strong hurricans\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "wind_speed_var = \"wmo_wind\"  \n",
    "pressure_var = \"wmo_pres\"  \n",
    "\n",
    "tks_na = tks_na.where(\n",
    "    (tks_na[wind_speed_var] >= 96) | (tks_na[pressure_var] <= 960), drop=True\n",
    ")\n",
    "tks_na"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ad7463-d32a-4061-98bd-7bca0fb7fe95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from kneed import KneeLocator\n",
    "from scipy.stats import skew\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "def analyze_hurricane_tracks_with_skewness(tks_na, n_clusters=None, max_k=15, num_interp_points=50):\n",
    "    \"\"\"\n",
    "    使用 **三阶矩（偏度 Skewness）** 进行聚类分析，并可视化飓风轨迹、起点、平均轨迹。\n",
    "    \n",
    "    :param tks_na: xarray.Dataset，包含飓风轨迹数据。\n",
    "    :param n_clusters: int，K-Means 聚类数（若为 None，则使用肘部法则自动确定）。\n",
    "    :param max_k: int，肘部法则的最大 K 值，仅在 n_clusters=None 时使用。\n",
    "    :param num_interp_points: int，轨迹插值点数，用于平滑平均轨迹。\n",
    "    :return: DataFrame，包含聚类结果、风暴数统计、经纬度偏度统计。\n",
    "    \"\"\"\n",
    "\n",
    "    moments = {}\n",
    "    genesis_points = {}\n",
    "\n",
    "    for storm_id in tks_na.storm:\n",
    "        lon, lat = tks_na.sel(storm=storm_id).lon.values, tks_na.sel(storm=storm_id).lat.values\n",
    "        valid_mask = ~np.isnan(lon) & ~np.isnan(lat)\n",
    "        lon, lat = lon[valid_mask], lat[valid_mask]\n",
    "\n",
    "        if len(lon) == 0 or len(lat) == 0:\n",
    "            continue\n",
    "\n",
    "        # 计算三阶矩（偏度）\n",
    "        skew_vector = np.array([skew(lon), skew(lat)])\n",
    "        \n",
    "        moments[storm_id.values.item()] = skew_vector\n",
    "        genesis_points[storm_id.values.item()] = (lon[0], lat[0])  # 记录 Genesis 位置\n",
    "\n",
    "    df_moments = pd.DataFrame.from_dict(moments, orient='index', columns=[\"Skew_Lon\", \"Skew_Lat\"])\n",
    "    df_genesis = pd.DataFrame.from_dict(genesis_points, orient='index', columns=[\"Genesis_Lon\", \"Genesis_Lat\"])\n",
    "\n",
    "    # **Step 2: 聚类**\n",
    "    df_scaled = StandardScaler().fit_transform(df_moments)\n",
    "\n",
    "    if n_clusters is None:\n",
    "        inertia = [KMeans(n_clusters=k, random_state=42, n_init=10).fit(df_scaled).inertia_ for k in range(1, max_k+1)]\n",
    "        n_clusters = KneeLocator(range(1, max_k+1), inertia, curve=\"convex\", direction=\"decreasing\").knee\n",
    "\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "    df_moments[\"Cluster\"] = kmeans.fit_predict(df_scaled) + 1  # 让 Cluster 从 1 开始\n",
    "\n",
    "    # **合并 Genesis 位置**\n",
    "    df_moments = df_moments.merge(df_genesis, left_index=True, right_index=True)\n",
    "\n",
    "    # **获取轨迹数据**\n",
    "    df_tracks = tks_na[['time', 'lat', 'lon']].to_dataframe().reset_index()\n",
    "    df_tracks = df_tracks.dropna(subset=['lat', 'lon']).rename(columns={'storm': 'storm_id'})\n",
    "    df_tracks = df_tracks.merge(df_moments[['Cluster']], left_on='storm_id', right_index=True, how='left')\n",
    "\n",
    "    # **计算平均 Genesis 位置**\n",
    "    df_cluster_centers = df_moments.groupby(\"Cluster\")[[\"Genesis_Lon\", \"Genesis_Lat\"]].mean()\n",
    "\n",
    "    # **计算平滑的平均轨迹**\n",
    "    def compute_smoothed_average_tracks(df_tracks, num_points=num_interp_points):\n",
    "        df_tracks[\"time_index\"] = df_tracks.groupby(\"storm_id\").cumcount()\n",
    "        df_tracks[\"time_norm\"] = df_tracks[\"time_index\"] / df_tracks.groupby(\"storm_id\")[\"time_index\"].transform(\"max\")\n",
    "\n",
    "        time_bins = np.linspace(0, 1, num_points)\n",
    "        avg_tracks = []\n",
    "\n",
    "        for cluster_id in sorted(df_tracks[\"Cluster\"].unique()):\n",
    "            cluster_tracks = df_tracks[df_tracks[\"Cluster\"] == cluster_id]\n",
    "\n",
    "            avg_lon, avg_lat = [], []\n",
    "            for t in time_bins:\n",
    "                subset = cluster_tracks.iloc[(cluster_tracks[\"time_norm\"] - t).abs().argsort()[:10]]\n",
    "                avg_lon.append(subset[\"lon\"].mean())\n",
    "                avg_lat.append(subset[\"lat\"].mean())\n",
    "\n",
    "            avg_tracks.append(pd.DataFrame({\"Cluster\": cluster_id, \"lon\": avg_lon, \"lat\": avg_lat, \"time_norm\": time_bins}))\n",
    "\n",
    "        return pd.concat(avg_tracks)\n",
    "\n",
    "    avg_tracks = compute_smoothed_average_tracks(df_tracks)\n",
    "\n",
    "    # **绘制轨迹聚类结果**\n",
    "    fig, axes = plt.subplots((n_clusters+1)//2, 2, figsize=(12, 10), subplot_kw={'projection': ccrs.PlateCarree()})\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, cluster_id in enumerate(sorted(df_moments[\"Cluster\"].unique())):\n",
    "        ax = axes[i]\n",
    "        ax.set_extent([-100, 20, 0, 50], crs=ccrs.PlateCarree())\n",
    "        ax.add_feature(cfeature.COASTLINE, linewidth=0.8)\n",
    "        ax.add_feature(cfeature.BORDERS, linestyle=':')\n",
    "\n",
    "        subset = df_tracks[df_tracks[\"Cluster\"] == cluster_id]\n",
    "\n",
    "        for track_id in subset[\"storm_id\"].unique():\n",
    "            track_data = subset[subset[\"storm_id\"] == track_id]\n",
    "            ax.plot(track_data[\"lon\"], track_data[\"lat\"], color=\"gray\", alpha=0.2, linewidth=0.5)\n",
    "\n",
    "        gen_lon, gen_lat = df_cluster_centers.loc[cluster_id]\n",
    "        ax.scatter(gen_lon, gen_lat, edgecolors=\"red\", facecolors=\"none\", marker=\"o\", s=100, linewidth=2, label=\"Avg Genesis\")\n",
    "\n",
    "        mean_track = avg_tracks[avg_tracks[\"Cluster\"] == cluster_id]\n",
    "        ax.plot(mean_track[\"lon\"], mean_track[\"lat\"], color=\"orange\", linewidth=2, linestyle=\"-\", alpha=0.8, label=\"Avg Track\")\n",
    "\n",
    "        ax.set_title(f\"Cluster {cluster_id}\")\n",
    "        ax.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # **计算每个类别的统计信息**\n",
    "    cluster_stats = df_moments.groupby(\"Cluster\").agg(\n",
    "        Num_Hurricanes=pd.NamedAgg(column=\"Skew_Lon\", aggfunc=\"count\"),\n",
    "        Avg_Skew_Lon=pd.NamedAgg(column=\"Skew_Lon\", aggfunc=\"mean\"),\n",
    "        Avg_Skew_Lat=pd.NamedAgg(column=\"Skew_Lat\", aggfunc=\"mean\")\n",
    "    )\n",
    "\n",
    "    return df_moments, df_tracks, cluster_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6af1613-ae0e-4e63-a898-0a121837a002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1️⃣ Cold Phase\n",
    "tks_na_cold = tks_na.where(\n",
    "    (tks_na.time >= np.datetime64(\"1965-01-01\")) & \n",
    "    (tks_na.time <= np.datetime64(\"1995-12-31\")), \n",
    "    drop=True\n",
    ")\n",
    "\n",
    "# 2️⃣ 运行聚类分析（使用偏度 Skewness 进行聚类）\n",
    "df_moments, df_tracks, cluster_stats = analyze_hurricane_tracks_with_skewness(\n",
    "    tks_na_cold,  # 输入数据\n",
    "    n_clusters=None,  # 自动确定最佳 K\n",
    "    max_k=10,  # 最大聚类数\n",
    "    num_interp_points=50  # 轨迹平滑插值点\n",
    ")\n",
    "\n",
    "print(cluster_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb85b9bc-f13a-4153-a990-8159afc5c716",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 统计数据（从 DataFrame `cluster_stats` 获取）\n",
    "clusters = cluster_stats.index.to_numpy()  # 获取 Cluster 索引\n",
    "skew_lon = cluster_stats[\"Avg_Skew_Lon\"].to_numpy()  # 获取经度偏度\n",
    "skew_lat = cluster_stats[\"Avg_Skew_Lat\"].to_numpy()  # 获取纬度偏度\n",
    "num_hurricanes = cluster_stats[\"Num_Hurricanes\"].to_numpy()  # 获取飓风数量\n",
    "\n",
    "# 创建图形\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "# 绘制向量\n",
    "ax.quiver(\n",
    "    np.zeros_like(skew_lon), np.zeros_like(skew_lat),  # 起点 (0,0)\n",
    "    skew_lon, skew_lat,  # 终点 (Skew_Lon, Skew_Lat)\n",
    "    angles='xy', scale_units='xy', scale=1, color=['r', 'g', 'b', 'orange'], width=0.005\n",
    ")\n",
    "\n",
    "# 添加标签（包含 Cluster 编号和飓风数量）\n",
    "for i, (x, y) in enumerate(zip(skew_lon, skew_lat)):\n",
    "    ax.text(x, y, f\"Cluster {clusters[i]} ({num_hurricanes[i]})\", fontsize=10, ha='left', va='bottom')\n",
    "\n",
    "# 设置坐标轴\n",
    "ax.axhline(0, color='gray', linewidth=0.5)  \n",
    "ax.axvline(0, color='gray', linewidth=0.5)  \n",
    "ax.set_xlim(-1.5, 1.5)\n",
    "ax.set_ylim(-1.5, 1.5)\n",
    "ax.set_xlabel(\"Avg Skew Lon\")\n",
    "ax.set_ylabel(\"Avg Skew Lat\")\n",
    "ax.set_title(\"Hurricane Cluster Skewness Vectors Cold\")\n",
    "\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d4a2cc-281e-4fde-876a-2a8c430eb620",
   "metadata": {},
   "outputs": [],
   "source": [
    "tks_na_warm = tks_na.where(\n",
    "    (tks_na.time >= np.datetime64(\"1995-01-01\")) & \n",
    "    (tks_na.time <= np.datetime64(\"2025-12-31\")), \n",
    "    drop=True\n",
    ")\n",
    "\n",
    "# 2️⃣ 运行聚类分析（使用偏度 Skewness 进行聚类）\n",
    "df_moments, df_tracks, cluster_stats = analyze_hurricane_tracks_with_skewness(\n",
    "    tks_na_warm,  # 输入数据\n",
    "    n_clusters=None,  # 自动确定最佳 K\n",
    "    max_k=10,  # 最大聚类数\n",
    "    num_interp_points=50  # 轨迹平滑插值点\n",
    ")\n",
    "\n",
    "# 3️⃣ 查看每个聚类的统计结果\n",
    "print(cluster_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd15816-8904-4230-8b40-dc4684f7aba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 统计数据（从 DataFrame `cluster_stats` 获取）\n",
    "clusters = cluster_stats.index.to_numpy()  # 获取 Cluster 索引\n",
    "skew_lon = cluster_stats[\"Avg_Skew_Lon\"].to_numpy()  # 获取经度偏度\n",
    "skew_lat = cluster_stats[\"Avg_Skew_Lat\"].to_numpy()  # 获取纬度偏度\n",
    "num_hurricanes = cluster_stats[\"Num_Hurricanes\"].to_numpy()  # 获取飓风数量\n",
    "\n",
    "# 创建图形\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "# 绘制向量\n",
    "ax.quiver(\n",
    "    np.zeros_like(skew_lon), np.zeros_like(skew_lat),  # 起点 (0,0)\n",
    "    skew_lon, skew_lat,  # 终点 (Skew_Lon, Skew_Lat)\n",
    "    angles='xy', scale_units='xy', scale=1, color=['r', 'g', 'b', 'orange'], width=0.005\n",
    ")\n",
    "\n",
    "# 添加标签（包含 Cluster 编号和飓风数量）\n",
    "for i, (x, y) in enumerate(zip(skew_lon, skew_lat)):\n",
    "    ax.text(x, y, f\"Cluster {clusters[i]} ({num_hurricanes[i]})\", fontsize=10, ha='left', va='bottom')\n",
    "\n",
    "# 设置坐标轴\n",
    "ax.axhline(0, color='gray', linewidth=0.5)  \n",
    "ax.axvline(0, color='gray', linewidth=0.5)  \n",
    "ax.set_xlim(-1.5, 1.5)\n",
    "ax.set_ylim(-1.5, 1.5)\n",
    "ax.set_xlabel(\"Avg Skew Lon\")\n",
    "ax.set_ylabel(\"Avg Skew Lat\")\n",
    "ax.set_title(\"Hurricane Cluster Skewness Vectors Warm\")\n",
    "\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c83a6c-7b05-45ba-9287-d657b0add80e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d6a32154-c38e-45b5-ba69-a725fe1ccfbe",
   "metadata": {},
   "source": [
    "## **5. Limitations and Future Improvements**\n",
    "\n",
    "Hurricane formation is a complex process influenced by a multitude of factors, including **sea surface temperature (SST), atmospheric pressure patterns, wind shear, humidity levels, and ocean currents**. However, in this study, we focused solely on the effects of **AMO (Atlantic Multidecadal Oscillation) and ENSO (El Niño-Southern Oscillation)** due to time constraints and data availability.\n",
    "\n",
    "While AMO and ENSO play significant roles in modulating hurricane activity, they **do not account for all variability** observed in hurricane frequency, intensity, or trajectory. Some key limitations of our study include:\n",
    "\n",
    "- **Exclusion of Other Climate Drivers**:  \n",
    "  - Factors such as the **Madden-Julian Oscillation (MJO)**, **North Atlantic Oscillation (NAO)**, and **Pacific Decadal Oscillation (PDO)** can also influence hurricane activity but were not considered in this analysis.\n",
    "  - Regional atmospheric conditions, such as **wind shear and humidity variations**, can significantly impact hurricane formation and intensification.\n",
    "\n",
    "- **Simplified Track Classification Approach**:  \n",
    "  - Our clustering model is based on **hurricane track moments (centroid, variance, skewness, and kurtosis)**, but additional meteorological parameters (e.g., pressure changes, vorticity, and energy dissipation) could refine the classification.\n",
    "\n",
    "- **Data and Computational Constraints**:  \n",
    "  - Our dataset was limited to **historical observations**. Incorporating **real-time forecasting data** and **higher-resolution climate models** could enhance predictive accuracy.\n",
    "  - Computational limitations restricted our ability to apply **deep learning models** for hurricane classification and forecasting.\n",
    "\n",
    "Although our study highlights the significant influence of AMO and ENSO on hurricanes, it is crucial to acknowledge that **hurricane formation and evolution result from a complex interplay of multiple atmospheric and oceanic factors**. Due to time constraints, we have not yet explored these additional influences. However, if given the opportunity, we plan to conduct a **comprehensive multi-factor analysis** to enhance our understanding of hurricane dynamics and refine our predictive models. **By incorporating more variables and improving methodologies, we aim to achieve higher forecasting accuracy, ultimately contributing to better disaster preparedness and mitigation strategies.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3defe9ee-c951-4c6b-b265-7331013894db",
   "metadata": {},
   "source": [
    "## **Conclusion**\n",
    "> *\"The direction of the storm is never truly random.\"*  \n",
    "\n",
    "By leveraging **higher-order moment analysis and K-Means clustering**, this study uncovers the crucial roles AMO and ENSO play in hurricane formation, intensity, and trajectory. With advanced data analysis and predictive modeling, we may soon achieve greater accuracy in forecasting hurricanes—providing valuable insights for disaster prevention and mitigation.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f0b748-151b-4531-8384-2429a25e04d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cadc579-3431-4167-9ef2-6a84faad0353",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcee4021-d98b-4b29-aab8-5d9a9a6bf623",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
